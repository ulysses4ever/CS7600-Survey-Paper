\documentclass[11pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{xspace}

\newcommand{\fix}{[{\color{red}\bf FIX!}]\xspace}

\title{CS7600: Survey Paper\\Lazy Functional Programming on the Multiprocessor}
\author{Artem Pelenitsyn}
\date{November 2018}

\usepackage{fullpage}

\begin{document}

\maketitle

\begin{abstract}
Purely functional programming should be an ideal target for parallelizationâ€¦ in theory. A little number of actual implementations have made it into wide use. Haskell and its most popular compiler, GHC, is one of such developments. The papers under consideration tell the story of Haskell going to the multiprocessor.
\end{abstract}


\section{Introduction}

A prominent family of modern programming languages is that of functional programming languages. In such languages, the main tool for organizing code is a notion of a pure function. This kind of function always produce the same result given the same input. Pure functions, therefore, do not use a mutable state or side-effecting operations. These features suggest an especially attractive area for parallel and concurrent programming (called \textit{parallel} below, for short).

%{\bf Introduce concept of lazy functional languages}
Further, lazy functional programming objectifies a notion of yet-to-be-performed computation, called \emph{thunk} in certain languages (such as Haskell, as we will see). Thunks store function applications that have results not retrieved so far. A little reflection on this notion quickly jumps to the conclusion that a thunk is not so much different from a \emph{spark} (also known as a future or promise)~--- a computation that is scheduled for an asynchronous execution and that could be forced at a later moment in the program.

Unfortunately, the past performance of functional languages has not always been up to the requirements of industry.
Modern functional programming language research is now targeting both research and industrial applications.

From early on, the developers of the major Haskell implementation, the GHC compiler, tried to utilize the idea of parallel functional programming.
Parallel implementations are important today in order to reach the highest levels of performance. 

Parallel functional programming poses interesting challenges in the field of systems and language implementation.
First, design decisions have to keep in mind particular hardware architectures which the compiler targets (Section 2).
Second, a binding associated with a thunk may be retrieved from different threads. Concurrent thunk access requires special care from the side of the scheduler (Section 3). Third, a reasonable garbage collection strategy should take advantage of multiple capabilities dealing with the heap in a clustered manner (Section 4).
%Fourth, a naive approach to sparking threads could lead to severe problems with the granularity of parallelism. %The issue should be addressed by a dedicated load balancing logic in the run-time (Section 5).

\section{Programming Model and Target Architectures}
\label{sec:model}

% Polish this paragraph later
A lazy, functional, parallel programming language must specify both a programming model and how it will be applied to the target architectures.
Any parallel target architecture must introduce the concept of a processing element (PE). A programming language should supply a way to utilize PEs for a programmer.
Programmer-provided annotations on bindings are a way to achieve parallelism in a lazy functional language. The annotation signals that the PE executing the program will put the annotated thunk in its spark queue. The queue will be used to spread the job between all PEs later on, when the PE will interrupt from executing the program.

This approach to parallel computations is sometimes called semi-explicit because, although code has to be modified to run in parallel, modifications are rather small, straight-forward and do not require explicit management of threads.

The basic framework for implementing parallel functional programming was laid out by~\cite{Trinder96}. The execution model there forms a network of virtual processing elements communicating via message-passing, asynchronously. Each PE has its own spark queue that it schedules for execution in separate threads, in FIFO order, non-preemptively. The scheduling scheme allows context switches only when a thread blocks, finishes or run out of memory. When PE finishes, it attempts to schedule the next spark in the queue. If the queue is empty, the PE can inquire about sparks available on other PEs via messaging.

The most flexible and most accessible architecture in the 90s was that of the cluster. Naturally, a pioneering implementation of the parallel lazy functional programming model~\cite{Trinder96} used message passing and distributed memory interface of PVM as a back-end. Middleware, such as PVM or MPI, hides many challenges in implementation of parallel programming model behind a high-level interface. Also, a messaging system is scheduled preemptively by an operating system and, therefore, allows the runtime system of a parallel language to go with a simplified non-preemptive approach.

Over the years, it became apparent that having robust a compiler-embedded solution for any kind of distributed-memory architecture is not feasible. At the same time, desktop computers received multiprocessors, which posed a similar challenge but in a much more uniform setting~\cite{Harris05}. Therefore, the paper investigates the design space outlined by~\cite{Trinder96} in a different direction. 

Switching from a cluster to a multicore processor means, for one, the absence of message-passing middleware that played crucial role in many management tasks. Consider thunk distribution. In a message-passing system, an idle processor could simply send messages requesting more job, at random. This ``pull'' model proved to be robust. On the other hand, on a multiprocessor without messaging infrastructure, an early solution to the problem of thunk distribution implemented a more expensive ``push'' model: when a busy PE pause executing a thread, it tries to find an idle PE to hand it in a free spark if it has any. Obviously, this puts additional burden on a wrong actor: the busy one, instead of the idle one. The idea to use a work-stealing queue data structure switched the responsibility, allowing an idle PE to search for a new job~\cite{Marlow09}. The data structure requires very modest synchronization and achieves notable speedups. 

\section{Parallel Thunk Evaluation}

Recall from Section~\ref{sec:model} the concept of a spark that provides a scheduling scheme~\cite{Trinder96} for thunks.
We consider a thunk as a dynamic data structure representing a pure functional term. Evaluating such structures should not have side effects visible in the program, but the data structure itself is updated during evaluation, namely, the computed value replaces the code used to compute this value.

A notable challenge arising from the scheduling scheme~\cite{Trinder96} is that one thunk can be entered by different threads. The solution to this problem employs so called \emph{black holes}. The black hole is a special value assigned to a thunk by an entering thread. Any other thread trying to execute the same thunk while its blackholed blocks and attaches itself to the queue of threads waiting on this black hole. When the first thread finishes executing the thunk, it updates the value of the thunk and moves all waiting threads to the runnable queue managed by the current PE.

The costs of fully synchronized evaluation of thunks described above become more significant when measured on a single multiprocessor. SMP processors became ubiquitous during the 2000s, so the blackholing approach to synchronization had to be reconsidered.

Consider precisely what happens if two threads start evaluating the same thunk. In the absence of side effects in the language, the result of both evaluations should be the same. The problem of synchronization still persists because both threads attempt to update the thunk with the (same) result of evaluation. Then, without additional arrangements, the threads would enter a race condition. Therefore, some amount of synchronization is needed even in the lock-free approach.

Concurrent evaluation of the same thunk wastes computational resources. Is this waste significant? Most of the thunks are cheap and the likelihood of concurrent evaluation is low. Nevertheless, we would like to have a guarantee that two threads do not commit to the same expensive computation. The mechanism providing the guarantee is (again) based on blackholing. 

Next, we provide details on the blackholing mechanism to make concurrent evaluation efficient.
Every thread, when it starts to execute a thunk, places an update frame on its own stack. The update frame refers to the thunk under evaluation and holds the code that performs the actual updating of the thunk with a computed value. If the evaluation takes ``too long'' (meaning: after some timeout), a thread switches from thunk evaluation to a search for update frames on its stack and tries to claim them. The claiming is implemented with \textit{compare-and-swap} instruction. Upon successful claiming, the thunk gets blackholed, and no other thread can ever start evaluating the thunk. In the case of an unsuccessful claim of a thunk, the thread realizes that some other thread have entered the thunk (and maybe already have finished working on it). There are two conclusions from that: first, all the update frames on top of the found one can be discarded as duplicate work; second, the thread itself should just block so that when it awakes it enters the thunk and just gets its value (computed by other thread) or blocked again (giving the other thread more time to finish computation of the thunk).

However, we cannot allow concurrent evaluation of thunks unless we can be assured that the thunks do not produce side effects.
When we talk about an absence of side effects, we actually mean that compiler statically knows what parts of the program can produce side effects and forbids scheduling them for asynchronous execution accordingly. The knowledge comes from the type system. Invalidating this assumption, the GHC compiler supports \texttt{unsafePerformIO} primitive, which allows a side effecting operation to escape the type check. In order to make such escape safe under concurrent evaluation, it was proposed to add one more primitive in the language~\cite{Harris05}. The primitive \texttt{justOnce} with a trivial function type \texttt{a -> a} would not have any operational content, but only mark the computations which should be executed not more than once. The programmer should use the marker when they know about hidden side effects inside a seemingly pure computation.

\section{Memory Management}

Memory management is based on PE-local GC-managed heaps. This approach facilitates local garbage collection and keeps its cost low. Some memory address bookkeeping is necessary to allow for thunks travelling between PEs. In a nutshell, every object can have two kinds of addresses, a local and a global one. A local address is only visible to the PE holding the object. A global address is used by other PEs to look up the object. A global address consists of a name of the owning PE and a local identifier. Note that for local garbage collection to be transparent from the outside, local identifier should not be exactly the local address because the latter may change after GC. Instead, there is a PE-local table mapping local identifiers to local addresses. Also, to track which local objects have been globalized, and to cache objects, some additional tables are built. Finally, moving individual objects between nodes is vastly inefficient. Therefore, the runtime tries to pack reasonably many ``nearby'' objects to the one in move.

\section{Measurements}

The initial work on parallel lazy functional programming~\cite{Trinder96}, put a limited effort into benchmarking the implementation, taking divide-and-conquer factorial algorithm as its case study. The evaluation section in~\cite{Harris05} is more substantial: it reports on positive results when experimenting with the sequential \texttt{nofib} test suite and parallel GHC driver.

\section{Conclusion}


\nocite{*}
\bibliographystyle{unsrt}
\bibliography{references}


\end{document}
