\documentclass[11pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{xspace}

\newcommand{\fix}{[{\color{red}\bf FIX!}]\xspace}

\title{CS7600: Survey Paper\\Lazy Functional Programming on Multiprocessor}
\author{Artem Pelenitsyn}
\date{November 2018}

\usepackage{fullpage}

\begin{document}

\maketitle

\begin{abstract}
Purely functional programming should be an ideal target for parallelizationâ€¦ in theory. A little number of actual implementations have made it into wide use. Haskell and its most popular compiler, GHC, is one of such developments. The papers under consideration tell the story of Haskell going to the multiprocessor.
\end{abstract}


\nocite{*}
\bibliographystyle{unsrt}
\bibliography{references}

\section{Introduction}

A wide range of modern programming languages can be divided into several families according to certain features they expose. A prominent family is that of functional programming languages. The main tool for organizing code in such languages is a notion of pure function. This kind of functions always produce the same result given the same input, and, therefore, use no mutable state. The absence of mutable state suggests especially attractive area for parallel and concurrent programming (called parallel below, for short).

Haskell is a modern functional programming language targeting both research and industrial applications. From early on, the developers of the major Haskell implementation, the GHC compiler, tried to utilize the idea of parallel functional programming. What makes Haskell special though is laziness. Lazy functional programming objectifies a notion of yet-to-be-performed computation, called \emph{thunk} in certain languages like Haskell. Thunks are used to store function applications whose results have not been retrieved so far. A little reflection on this notion quickly jumps to the conclusion that a thunk is not so much different from a \emph{spark} (also known as \emph{future} or \emph{promise})~--- a computation that is scheduled for an asynchronous execution and can be forced at a later moment in the program.

Being rather cheap at the surface language level, parallel functional programming poses interesting challenges in the field of systems and language implementation. First, design decisions have to keep in mind particular hardware architectures which the compiler targets (Section 1). Second, a binding associated with a thunk may be retrieved from different threads. This requires special care from the side of scheduler (Section 2). Third, a reasonable garbage collection strategy should take advantage of multiple capabilities dealing with the heap in a clustered manner (Section 3). Fourth, a naive approach to sparking threads could lead to severe problems with the granularity of parallelism. The issue should be addressed by a dedicated load balancing logic in the run-time (Section 4).

\section{Programming Model and Target Architectures}

The basic framework for implementing parallel functional programming was laid out by~\cite{Trinder96}. The execution model there forms a network of virtual processing elements (PE) communicating via message-passing, asynchronously. Each PE has a pool of runnable threads which it schedules in FIFO order, non-preemptively. The scheduling scheme allows context switches only when a thread blocks, finishes or run out of memory. A thread can be created from any spark if the corresponding thunk has not been computed already. 

The parallelism in the system stems from programmer-provided annotations. The annotation signals that the PE executing the program will put the annotated thunk in its spark queue. When the PE has nothing else to do, it attempts to schedule next spark in the queue. If the queue is empty, the PE can inquire about sparks available on other PEs via messaging.

This approach to parallel computations is sometimes called semi-explicit because, although code has to be modified to run in parallel, modifications are rather small, straight-forward and don't require explicit management of threads.

The most flexible and most accessible architecture in the 90s was that of the cluster. Naturally, a pioneering implementation of parallel lazy functional programming model~\cite{Trinder96} used message passing and distributed memory interface of PVM as a back-end. The upcoming era of shared-memory symmetric multiprocessors is declared in~\cite{Harris05}. Therefore, the paper investigates the design space outlined by~\cite{Trinder96} in a different direction. The main contribution there is a lock-free thunk evaluation algorithm for the shared heap.

\section{Parallel Thunk Evaluation}

We consider a thunk as a dynamic data structure representing a pure functional term. Evaluating such structure should not have side effects visible in the program, but the data structure itself is updated during evaluation, namely, the computed value replaces the code used to compute this value.

A notable challenge arising from the scheduling algorithm~\cite{Trinder96} is that one thunk can be entered by different threads. The solution to this problem employs so called \emph{back holes}. The black hole is a special value assigned to the a thunk by the entering thread. Any other thread trying to execute the same thunk while its blackholed blocks and attaches itself to the queue of threads waiting on this black hole. When the first thread finishes executing the thunk, it updates the value of the thunk and moves all waiting threads to the runnable queue managed by the current PE.

The costs of fully synchronized evaluation of thunks described above becomes more significant when measured on a single multiprocessor. SMP processors had become ubiquitous during the 2000s, so the blackholing approach to synchronization had to be reconsidered.

What happens if two threads start evaluating the same thunk? In the absence of side effects in the language, the result of both evaluations should be the same. So, maybe it is not that bad to allow work duplication? The problem of synchronization still persists, though, because both treads are going to try to update the thunk with the result of evaluation. Then, without additional arrangements, the threads would enter a race condition. Therefore, some amount of synchronization is needed even in the lock-free approach.

Concurrent evaluation of the same thunk wastes computational resources. Is this waste crucial? Most of the thunks are cheap and the likelihood of concurrent evaluation is low. Nevertheless, we would like to have a guarantee that two threads do not commit into the same expensive computation. The mechanism providing the guarantee is based on blackholing (again). 

In more detail, every thread, when start executing a thunk, places an update frame on its own stack. The update frame refers to the thunk under evaluation and holds the code which performs actual updating of the thunk with a computed value. If the evaluation takes ``too long'' (meaning: after some timeout), a thread switches from thunk evaluation to a search for update frames on its stack and tries to claim them. The claiming is implemented with \textit{compare-and-swap} instruction. Upon successful claiming, the thunk gets balckholed, and no other thread can ever start evaluating the thunk. In the case of an unsuccessful claim of a thunk, the thread realizes that some other thread have entered the thunk (and maybe already have finished working on it). There are two conclusion from that: first, all the update frames on top of the found one can be discarded as duplicate work, second, the thread itself should just block so that when it awakes it enters the thunk and just gets its value (computed by other thread) or blocked again (giving the other thread more time to finish computation of the thunk).

When we talk about absence of side effects, we actually mean that compiler statically knows what parts of the program can produce side effects and forbids scheduling them for asynchronous execution accordingly. The knowledge comes from the type system. Invalidating this assumption, the GHC compiler supports \texttt{unsafePerformIO} primitive which allows a side effecting operation to escape the type check. In order to make such escape safe under concurrent evaluation, it was proposed to add one more primitive in the language. The primitive \texttt{justOnce} with a trivial function type \texttt{a -> a} would not have any operational content, but only mark the computations which should be executed not more than once. The programmer should use the marker when they know about hidden side effects inside a seemingly pure computation.

\section{Memory Management}

Memory management is based on PE-local GC-managed heaps. This approach facilitates local garbage collection and keeps its cost low. Some memory address bookkeeping is necessary to allow for thunks travelling between PEs. In a nutshell, every object can have two kinds of addresses, a local and a global one. A local address is only visible to the PE holding the object. A global address is used by other PEs to lookup the object. A global address consists of a name of the owning PE and a local identifier. Note that for local garbage collection to be transparent from outside, local identifier should not be exactly the local address because the latter may change after GC. Instead, there is a PE-local table mapping local identifiers to local addresses. Also, to track which local objects have been globalized, and to cache objects, some additional tables are built. Finally, moving individual objects between nodes is vastly inefficient. Therefore, the runtime tries to pack reasonably many ``nearby'' objects to the one in move.

\section{Measurements}

The initial work on parallel lazy functional programming~\cite{Trinder96}, put a limited effort into benchmarking the implementation, taking divide-and-conquer factorial algorithm as its case study. The evaluation section in~\cite{Harris05} is more substantial: it reports on positive results when experimenting with the sequential \texttt{nofib} test suite and parallel GHC driver.

\section{Conclusion}

\end{document}
